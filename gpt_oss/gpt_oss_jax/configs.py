# Copyright 2025 The JAX Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import jax.numpy as jnp

from .model import Config

GPT_OSS_20B = Config(
    embed=2880,
    q_heads=64,
    kv_heads=8,
    num_layers=24,
    head_dim=64,
    vocab_size=201088,
    max_seq_len=1024,
    causal=True,
    sliding_attention_map=[
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
    ],
    sliding_window_size=128,
    moe_ffw_size=2880,
    moe_experts_per_tok=4,
    moe_num_experts=32,
    moe_gate_up_alpha=1.702,
    moe_gate_up_limit=7.0,
    moe_gate_dtype=jnp.float32,
    ep_strategy="decode",
    use_prefill_attn_kernel=False,
    use_decode_attn_kernel=False,
    use_ragged_dot_kernel=True,
    decode_ragged_dot_tiling={"block_compute": 32, "block_g": 1, "block_n": 1073741824, "block_out": 2048},
    dtype=jnp.bfloat16,
    norm_eps=1e-05,
    mesh=None,
    max_position_embeddings=131072,
    rope_theta=150000,
    rope_factor=32.0,
    rope_original_max_position_embeddings=4096,
    rope_beta_slow=1.0,
    rope_beta_fast=32.0,
    quant_moe=False,
    quant_attn=False,
    quant_cache=True,
    quant_scale_dtype=jnp.bfloat16,
    sample_topk=4,
    sample_temp=0.7,
)


GPT_OSS_120B = Config(
    embed=2880,
    q_heads=64,
    kv_heads=8,
    num_layers=36,
    head_dim=64,
    vocab_size=201088,
    max_seq_len=1024,
    causal=True,
    sliding_attention_map=[
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
        "sliding_attention",
        "full_attention",
    ],
    sliding_window_size=128,
    moe_ffw_size=2880,
    moe_experts_per_tok=4,
    moe_num_experts=128,
    moe_gate_up_alpha=1.702,
    moe_gate_up_limit=7.0,
    moe_gate_dtype=jnp.float32,
    ep_strategy="decode",
    use_prefill_attn_kernel=False,
    use_decode_attn_kernel=False,
    use_ragged_dot_kernel=True,
    decode_ragged_dot_tiling={"block_compute": 32, "block_g": 1, "block_n": 1073741824, "block_out": 2048},
    dtype=jnp.bfloat16,
    norm_eps=1e-05,
    mesh=None,
    max_position_embeddings=131072,
    rope_theta=150000,
    rope_factor=32.0,
    rope_original_max_position_embeddings=4096,
    rope_beta_slow=1.0,
    rope_beta_fast=32.0,
    quant_moe=False,
    quant_attn=False,
    quant_cache=True,
    quant_scale_dtype=jnp.bfloat16,
    sample_topk=4,
    sample_temp=0.7,
)
